{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Ipr6oTXs0QQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a series of transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Obtain and preprocess the QMNIST dataset\n",
        "train_dataset = datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.QMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Set up data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQIQDNlqtUR0",
        "outputId": "3d2a72d0-4e73-4ec6-8406-4480c938d2a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-train-images-idx3-ubyte.gz to ./data/QMNIST/raw/qmnist-train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9704059/9704059 [00:00<00:00, 35199463.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-train-images-idx3-ubyte.gz to ./data/QMNIST/raw\n",
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-train-labels-idx2-int.gz to ./data/QMNIST/raw/qmnist-train-labels-idx2-int.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 463024/463024 [00:00<00:00, 3588717.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-train-labels-idx2-int.gz to ./data/QMNIST/raw\n",
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-images-idx3-ubyte.gz to ./data/QMNIST/raw/qmnist-test-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9742279/9742279 [00:00<00:00, 29671373.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-test-images-idx3-ubyte.gz to ./data/QMNIST/raw\n",
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-labels-idx2-int.gz to ./data/QMNIST/raw/qmnist-test-labels-idx2-int.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 526800/526800 [00:00<00:00, 3852184.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-test-labels-idx2-int.gz to ./data/QMNIST/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a mapping of labels to digits\n",
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\"\n",
        "}\n",
        "\n",
        "# Define the transformations for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the QMNIST dataset for training\n",
        "train_dataset = datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create a figure to plot the images\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "columns, rows = 3, 3\n",
        "\n",
        "# Iterate through the dataset and visualize sample images\n",
        "for i in range(1, columns * rows + 1):\n",
        "    sample_index = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    image, label = train_dataset[sample_index]\n",
        "    figure.add_subplot(rows, columns, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "GW0hdB_ztkHV",
        "outputId": "22f6bff2-16db-4286-aa68-2f777e8bac9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for plotting\n",
        "plot_figure = plt.figure(figsize=(8, 8))\n",
        "columns, rows = 3, 3\n",
        "\n",
        "# Loop over the test dataset to display samples\n",
        "for i in range(1, columns * rows + 1):\n",
        "    sample_index = torch.randint(len(test_dataset), size=(1,)).item()\n",
        "    image, label = test_dataset[sample_index]\n",
        "    plot_figure.add_subplot(rows, columns, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "irPTFACGt0Rh",
        "outputId": "64bcf20d-4b87-42fa-95af-6dbec5bcc421"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture of the neural network\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the neural network model\n",
        "neural_network = MLP()\n"
      ],
      "metadata": {
        "id": "9u5igtlyuRqM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define your neural network architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize your neural network model\n",
        "model = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training Finished')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPtI-I5IuXTp",
        "outputId": "f995e6d8-e041-4559-c0b2-9bc5ebccf076"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.9954199111461639\n",
            "Epoch 1, Batch 200, Loss: 0.46690152496099474\n",
            "Epoch 1, Batch 300, Loss: 0.3955640897154808\n",
            "Epoch 1, Batch 400, Loss: 0.3240852090716362\n",
            "Epoch 1, Batch 500, Loss: 0.3118569702655077\n",
            "Epoch 1, Batch 600, Loss: 0.29150551185011864\n",
            "Epoch 1, Batch 700, Loss: 0.26566119089722634\n",
            "Epoch 1, Batch 800, Loss: 0.247981249243021\n",
            "Epoch 1, Batch 900, Loss: 0.2191791483014822\n",
            "Epoch 2, Batch 100, Loss: 0.20525375973433257\n",
            "Epoch 2, Batch 200, Loss: 0.21117535047233105\n",
            "Epoch 2, Batch 300, Loss: 0.19635077696293593\n",
            "Epoch 2, Batch 400, Loss: 0.17533508818596602\n",
            "Epoch 2, Batch 500, Loss: 0.17787901792675256\n",
            "Epoch 2, Batch 600, Loss: 0.1692874513939023\n",
            "Epoch 2, Batch 700, Loss: 0.1602169058471918\n",
            "Epoch 2, Batch 800, Loss: 0.1632572408951819\n",
            "Epoch 2, Batch 900, Loss: 0.1633285466581583\n",
            "Epoch 3, Batch 100, Loss: 0.1520793471299112\n",
            "Epoch 3, Batch 200, Loss: 0.13487318513914942\n",
            "Epoch 3, Batch 300, Loss: 0.14234700987115503\n",
            "Epoch 3, Batch 400, Loss: 0.1410978115350008\n",
            "Epoch 3, Batch 500, Loss: 0.1358002468943596\n",
            "Epoch 3, Batch 600, Loss: 0.12914741357788442\n",
            "Epoch 3, Batch 700, Loss: 0.11951927935704589\n",
            "Epoch 3, Batch 800, Loss: 0.13646985817700624\n",
            "Epoch 3, Batch 900, Loss: 0.11548842761665583\n",
            "Epoch 4, Batch 100, Loss: 0.10281039588153362\n",
            "Epoch 4, Batch 200, Loss: 0.09517962811514735\n",
            "Epoch 4, Batch 300, Loss: 0.10406465420499444\n",
            "Epoch 4, Batch 400, Loss: 0.11116095261648297\n",
            "Epoch 4, Batch 500, Loss: 0.10160695780999958\n",
            "Epoch 4, Batch 600, Loss: 0.11663227284327149\n",
            "Epoch 4, Batch 700, Loss: 0.10300338855944574\n",
            "Epoch 4, Batch 800, Loss: 0.11592751420103013\n",
            "Epoch 4, Batch 900, Loss: 0.11434764226898551\n",
            "Epoch 5, Batch 100, Loss: 0.08961458033882082\n",
            "Epoch 5, Batch 200, Loss: 0.0841465114755556\n",
            "Epoch 5, Batch 300, Loss: 0.08839141257107258\n",
            "Epoch 5, Batch 400, Loss: 0.08274201823398471\n",
            "Epoch 5, Batch 500, Loss: 0.1025930161587894\n",
            "Epoch 5, Batch 600, Loss: 0.08277657624334096\n",
            "Epoch 5, Batch 700, Loss: 0.09270825661718846\n",
            "Epoch 5, Batch 800, Loss: 0.09713385717011988\n",
            "Epoch 5, Batch 900, Loss: 0.09941659395582975\n",
            "Epoch 6, Batch 100, Loss: 0.07226854994893074\n",
            "Epoch 6, Batch 200, Loss: 0.07736334510147572\n",
            "Epoch 6, Batch 300, Loss: 0.07816485670860857\n",
            "Epoch 6, Batch 400, Loss: 0.08544026779942214\n",
            "Epoch 6, Batch 500, Loss: 0.07854282293003052\n",
            "Epoch 6, Batch 600, Loss: 0.08163695299066603\n",
            "Epoch 6, Batch 700, Loss: 0.08939330381341279\n",
            "Epoch 6, Batch 800, Loss: 0.07891419828403741\n",
            "Epoch 6, Batch 900, Loss: 0.08043307871557773\n",
            "Epoch 7, Batch 100, Loss: 0.06419948338530958\n",
            "Epoch 7, Batch 200, Loss: 0.06632711567450315\n",
            "Epoch 7, Batch 300, Loss: 0.06120630642864853\n",
            "Epoch 7, Batch 400, Loss: 0.05932479914277792\n",
            "Epoch 7, Batch 500, Loss: 0.06886531680822372\n",
            "Epoch 7, Batch 600, Loss: 0.08172344260616228\n",
            "Epoch 7, Batch 700, Loss: 0.0828033689269796\n",
            "Epoch 7, Batch 800, Loss: 0.06640140454750508\n",
            "Epoch 7, Batch 900, Loss: 0.08576457973569632\n",
            "Epoch 8, Batch 100, Loss: 0.05550917129497975\n",
            "Epoch 8, Batch 200, Loss: 0.052635156409814955\n",
            "Epoch 8, Batch 300, Loss: 0.05975543757434934\n",
            "Epoch 8, Batch 400, Loss: 0.062349347127601505\n",
            "Epoch 8, Batch 500, Loss: 0.07184697683900594\n",
            "Epoch 8, Batch 600, Loss: 0.07652409480651841\n",
            "Epoch 8, Batch 700, Loss: 0.06536867995280772\n",
            "Epoch 8, Batch 800, Loss: 0.06575225452426821\n",
            "Epoch 8, Batch 900, Loss: 0.0747364415973425\n",
            "Epoch 9, Batch 100, Loss: 0.051451422087848184\n",
            "Epoch 9, Batch 200, Loss: 0.06317048537079245\n",
            "Epoch 9, Batch 300, Loss: 0.0508653161150869\n",
            "Epoch 9, Batch 400, Loss: 0.0657229504082352\n",
            "Epoch 9, Batch 500, Loss: 0.0521457956568338\n",
            "Epoch 9, Batch 600, Loss: 0.05101973460521549\n",
            "Epoch 9, Batch 700, Loss: 0.07820402762154118\n",
            "Epoch 9, Batch 800, Loss: 0.06111700967885554\n",
            "Epoch 9, Batch 900, Loss: 0.0617571658687666\n",
            "Epoch 10, Batch 100, Loss: 0.04343584300950169\n",
            "Epoch 10, Batch 200, Loss: 0.05013045589264948\n",
            "Epoch 10, Batch 300, Loss: 0.04938091878313571\n",
            "Epoch 10, Batch 400, Loss: 0.05292755328351632\n",
            "Epoch 10, Batch 500, Loss: 0.0574540735501796\n",
            "Epoch 10, Batch 600, Loss: 0.05375046480912715\n",
            "Epoch 10, Batch 700, Loss: 0.053346109945559876\n",
            "Epoch 10, Batch 800, Loss: 0.06721250434406102\n",
            "Epoch 10, Batch 900, Loss: 0.05850490831071511\n",
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables for tracking correct predictions and total samples\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Turn off gradient computation\n",
        "with torch.no_grad():\n",
        "    # Iterate through the training dataset\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # Update the total number of samples\n",
        "        total += labels.size(0)\n",
        "        # Update the number of correctly classified samples\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct / total * 100\n",
        "print(f'Accuracy on the training set: {accuracy:.3f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uak49mnougkv",
        "outputId": "a9370f7d-3f6e-404d-9a48-46df1d18361e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set: 98.550%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Turn off gradient computation\n",
        "with torch.no_grad():\n",
        "    # Iterate through the test loader\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # Update the total number of samples\n",
        "        total += labels.size(0)\n",
        "        # Update the number of correctly classified samples\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct / total * 100\n",
        "print(f'Accuracy on the test set: {accuracy:.3f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuHqXQ7tvgsM",
        "outputId": "de40c82b-2243-46eb-ff3a-c72e321e8a49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 96.983%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize an empty list to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Initialize variables for tracking correct predictions and total samples\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Turn off gradient computation\n",
        "with torch.no_grad():\n",
        "    # Iterate through the test loader\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # Extend the predictions list with the predicted labels\n",
        "        predictions.extend(predicted.numpy())\n",
        "        # Update the total number of samples\n",
        "        total += labels.size(0)\n",
        "        # Update the number of correctly classified samples\n",
        "        correct += (predicted == labels).sum().item()\n"
      ],
      "metadata": {
        "id": "2cc8EjDfvsi6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one test image and its label\n",
        "image, label = images[0], labels[0]\n",
        "\n",
        "# Reshape the image tensor to a 28x28 shape\n",
        "image = image.view(28, 28)\n",
        "\n",
        "# Convert the image tensor to a numpy array for visualization\n",
        "image_numpy = image.numpy()\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_numpy, cmap='gray')\n",
        "plt.title(f'Predicted Label: {predictions[0]}, Actual Label: {label.item()}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "DisKwn3xv4uK",
        "outputId": "68c3d2ea-a0eb-40f6-c03f-a8dd4c31f525"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAalklEQVR4nO3ceXBV9f3/8ddFsgcUQ0CiIYRoRIlUGxcKlLCDbNNWasFWQaUGq0TbUuhQSmQplFoRBYzLDIRhq6YKaIugVFDMlCqLSyiUNSjEalgMCiRpks/vD795/7i5CeRcSALx+ZhhRm7O+57PJSd55twcj8855wQAgKQmDb0AAMCFgygAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhijUoXbt2mnUqFH29w0bNsjn82nDhg0Ntqaqqq6xPvTo0UMpKSnn9Tkb4nU0Zj169FCPHj3qdZ+jRo1SdHT0eX3OhngdF7tGG4Xs7Gz5fD77Ex4eruTkZD388MP6/PPPG3p5nqxevVqPPfZYg67B5/Pp4YcfbtA11JXHHnvM71ip+ic3N/ecnn/Hjh12DH755ZdBP8+MGTO0cuXKc1rL+dauXTsNHjy4oZdRZ2o6Jv74xz829NLqTNOGXkBdmzp1qhITE1VcXKx3331XWVlZWr16tfLy8hQZGVmva+nevbtOnTql0NBQT3OrV6/W/PnzGzwMjdWPfvQjXX311QGPT5w4UV9//bVuueWWc3r+JUuW6IorrtCxY8f017/+VaNHjw7qeWbMmKFhw4bpBz/4wTmtB9707dtX99xzj99jN910UwOtpu41+ijcfvvtuvnmmyVJo0ePVkxMjGbPnq1Vq1ZpxIgR1c6cOHFCUVFR530tTZo0UXh4+Hl/XpybTp06qVOnTn6Pffrppzp48KBGjx7tOeKnc85p2bJluuuuu7R//34tXbo06CigYSQnJ+tnP/tZQy+j3jTat49q0qtXL0nS/v37Jf3/9zH37t2rgQMHqlmzZvrpT38qSaqoqNCcOXPUsWNHhYeHq3Xr1kpPT9exY8f8ntM5p+nTp+uqq65SZGSkevbsqe3btwfsu6bfKfzrX//SwIED1aJFC0VFRalTp0566qmnbH3z58+X5H8qW+l8r/FcrFq1SoMGDVJcXJzCwsKUlJSkadOmqby8vNrtt2zZoi5duigiIkKJiYl69tlnA7YpKSlRZmamrr76aoWFhSk+Pl7jx49XSUnJWdezd+9e7d27N6jXsnz5cjnn7FgIVm5urvLz8zV8+HANHz5c77zzjg4ePBiwXUVFhZ566indcMMNCg8PV2xsrAYMGKDNmzdL+uZzf+LECS1atMiOgcrfoYwaNUrt2rULeM7Kt8VOt3DhQvXq1UutWrVSWFiYrr/+emVlZZ3TazybjRs36sc//rHatm1rn8Nf/vKXOnXqVLXb79u3T/3791dUVJTi4uI0depUVb2Zc22P++p88skn2rlzp6fXcOrUKRUXF3uauVg1+jOFqiq/ScTExNhjZWVl6t+/v7p166Y///nP9rZSenq6srOzde+99yojI0P79+/XvHnztG3bNuXm5iokJESSNHnyZE2fPl0DBw7UwIEDtXXrVvXr10+lpaVnXc+bb76pwYMHq02bNnrkkUd0xRVXaMeOHfrb3/6mRx55ROnp6SooKNCbb76pxYsXB8zXxxprKzs7W9HR0frVr36l6OhovfXWW5o8ebKOHz+uxx9/3G/bY8eOaeDAgbrzzjs1YsQIvfTSS3rwwQcVGhqq++67T9I3X/hDhw7Vu+++qwceeEDXXXedPv74Yz355JPatWvXWd9f7927tyQpPz/f82tZunSp4uPj1b17d8+zVZ8nKSlJt9xyi1JSUhQZGanly5frN7/5jd92999/v7Kzs3X77bdr9OjRKisr08aNG7Vp0ybdfPPNWrx4sUaPHq1bb71VDzzwgCQpKSnJ83qysrLUsWNHDR06VE2bNtVrr72mX/ziF6qoqNBDDz10Tq+1Jjk5OTp58qQefPBBxcTE6L333tPcuXN18OBB5eTk+G1bXl6uAQMGqHPnzvrTn/6kNWvWKDMzU2VlZZo6daptV9vjvjr33HOP3n777YDQ1CQ7O1vPPPOMnHO67rrrNGnSJN11113B/WNcDFwjtXDhQifJrVu3zhUWFrpPP/3U/eUvf3ExMTEuIiLCHTx40Dnn3MiRI50k99vf/tZvfuPGjU6SW7p0qd/ja9as8Xv8iy++cKGhoW7QoEGuoqLCtps4caKT5EaOHGmPrV+/3kly69evd845V1ZW5hITE11CQoI7duyY335Of66HHnrIVfepqos11kSSe+ihh864zcmTJwMeS09Pd5GRka64uNgeS0tLc5LcE088YY+VlJS4G2+80bVq1cqVlpY655xbvHixa9Kkidu4caPfcz777LNOksvNzbXHEhISAl5HQkKCS0hIOOtrqyovL89JcuPHj/c8e7rS0lIXExPjfve739ljd911l/vOd77jt91bb73lJLmMjIyA5zj98xUVFVXt52rkyJHVvs7MzMyA46a6z1H//v1d+/bt/R5LS0tzaWlp1bwqfwkJCW7QoEFn3Ka6fc6cOdP5fD534MABe6zya3Hs2LH2WEVFhRs0aJALDQ11hYWFzrnaH/c1vY7K4682unTp4ubMmeNWrVrlsrKyXEpKipPknnnmmVrNX4wa/dtHffr0UWxsrOLj4zV8+HBFR0drxYoVuvLKK/22e/DBB/3+npOTo0svvVR9+/bV4cOH7U9qaqqio6O1fv16SdK6detUWlqqsWPH+p2qP/roo2dd27Zt27R//349+uijuuyyy/w+VvW0vzr1sUYvIiIi7L+/+uorHT58WN///vd18uTJgNP1pk2bKj093f4eGhqq9PR0ffHFF9qyZYu9vuuuu04dOnTwe32VbwFWvr6a5OfnB32WIOmc3zp6/fXXdeTIEb/fXY0YMUIffvih31t3L7/8snw+nzIzMwOeozbHgRenf46Kiop0+PBhpaWlad++fSoqKjqv+6punydOnNDhw4fVpUsXOee0bdu2gO1Pv8qt8qq30tJSrVu3TlLtj/uabNiwodZnCbm5uXrkkUc0dOhQjRkzRlu2bFFKSoomTpxY49tfF7tG//bR/PnzlZycrKZNm6p169a69tpr1aSJfwubNm2qq666yu+x3bt3q6ioSK1atar2eb/44gtJ0oEDByRJ11xzjd/HY2Nj1aJFizOurfKtrGCv2a+PNXqxfft2TZo0SW+99ZaOHz/u97Gq33Di4uICfpmfnJws6Ztv5p07d9bu3bu1Y8cOxcbGVru/ytd3Prn/+8VwSkpKwC+fvVqyZIkSExMVFhamPXv2SPrmLZ/IyEgtXbpUM2bMkPTNcRAXF6fLL7/8nNd/Nrm5ucrMzNQ///lPnTx50u9jRUVFuvTSS8/7Pj/55BNNnjxZr776asB7/lWPiyZNmqh9+/Z+j51+XEi1P+7rQmhoqB5++GELRLdu3epsXw2l0Ufh1ltvtauPahIWFhYQioqKCrVq1cp+aqyqpm9U9elCWuOXX36ptLQ0NW/eXFOnTlVSUpLCw8O1detWTZgwQRUVFZ6fs6KiQjfccINmz55d7cfj4+PPddkBcnNzdeDAAc2cOfOcnuf48eN67bXXVFxcHBBjSVq2bJn+8Ic/nJczgZqeo+ov+Pfu3avevXurQ4cOmj17tuLj4xUaGqrVq1frySefDOpzdDbl5eXq27evjh49qgkTJqhDhw6KiorSoUOHNGrUqKCPi4Y87iuPu6NHj9bpfhpKo49CsJKSkrRu3Tp17drV7/S3qoSEBEnf/PRy+k84hYWFZ70SovIXhXl5eerTp0+N29X0RV8fa6ytDRs26MiRI3rllVf8fjlbeZVXVQUFBQGX/u7atUuS7EqapKQkffjhh+rdu/d5fxulJkuXLpXP5zvnXyS+8sorKi4uVlZWllq2bOn3sf/85z+aNGmScnNz1a1bNyUlJWnt2rU6evToGc8Wavo3aNGiRbX/U1zlGWKl1157TSUlJXr11VfVtm1be/xsb7eci48//li7du3SokWL/K71f/PNN6vdvqKiQvv27bOzA6n646I2x31d2bdvn6QL4wfDutDof6cQrDvvvFPl5eWaNm1awMfKysrsi7BPnz4KCQnR3Llz/d6nnDNnzln38d3vfleJiYmaM2dOwBf16c9V+Y2z6jb1scbauuSSSwLWXVpaqmeeeaba7cvKyvTcc8/5bfvcc88pNjZWqampkr55fYcOHdILL7wQMH/q1CmdOHHijGvyeknq//73P+Xk5Khbt25+3zSDsWTJErVv315jxozRsGHD/P6MGzdO0dHR9pPuHXfcIeecpkyZEvA8VY+D6r75JyUlqaioSB999JE99tlnn2nFihV+21X3OSoqKtLChQvP6bWeSXX7dM7ZJdfVmTdvnt+28+bNU0hIiF1NVtvjvia1vSS1sLAw4LGvvvpKc+bMUcuWLe04bWw4U6hBWlqa0tPTNXPmTH3wwQfq16+fQkJCtHv3buXk5Oipp57SsGHDFBsbq3HjxmnmzJkaPHiwBg4cqG3btun1118P+AmxqiZNmigrK0tDhgzRjTfeqHvvvVdt2rTRzp07tX37dq1du1aS7ODLyMhQ//79dckll2j48OH1ssbTbd68WdOnTw94vEePHurSpYtatGihkSNHKiMjQz6fT4sXL67xF3pxcXGaNWuW8vPzlZycrBdffFEffPCBnn/+ebuc8O6779ZLL72kMWPGaP369eratavKy8u1c+dOvfTSS1q7du0Z3xr0eknq2rVrdeTIkTP+grnyMsiFCxfWeK+lgoICrV+/XhkZGdV+PCwsTP3791dOTo6efvpp9ezZU3fffbeefvpp7d69WwMGDFBFRYU2btyonj172i9eU1NTtW7dOs2ePVtxcXFKTEzUbbfdpuHDh2vChAn64Q9/qIyMDJ08eVJZWVlKTk7W1q1bbb/9+vVTaGiohgwZovT0dH399dd64YUX1KpVK3322We1+jeqzp49e6o9Lm666Sb169dPSUlJGjdunA4dOqTmzZvr5ZdfrvEMNTw8XGvWrNHIkSN122236fXXX9ff//53TZw40X4yr+1xX5PaXpI6f/58rVy5UkOGDFHbtm312WefacGCBfrkk0+0ePHic/qfGi9oDXDFU72ovCT1/fffP+N2I0eOdFFRUTV+/Pnnn3epqakuIiLCNWvWzN1www1u/PjxrqCgwLYpLy93U6ZMcW3atHERERGuR48eLi8vL+AyyaqXpFZ69913Xd++fV2zZs1cVFSU69Spk5s7d659vKyszI0dO9bFxsY6n88XcDnd+VxjTSTV+GfatGnOOedyc3Nd586dXUREhIuLi3Pjx493a9euDXjNaWlprmPHjm7z5s3ue9/7ngsPD3cJCQlu3rx5AfstLS11s2bNch07dnRhYWGuRYsWLjU11U2ZMsUVFRXZdufjktThw4e7kJAQd+TIkRq3mTt3rpPk1qxZU+M2TzzxhJPk/vGPf9S4TXZ2tpPkVq1a5Zz75nP8+OOPuw4dOrjQ0FAXGxvrbr/9drdlyxab2blzp+vevbuLiIgIuJT4jTfecCkpKS40NNRde+21bsmSJdVekvrqq6+6Tp06ufDwcNeuXTs3a9Yst2DBAifJ7d+/37bzcklqTcfF/fff75xz7t///rfr06ePi46Odi1btnQ///nP3YcffugkuYULF9pzVX4t7t271/Xr189FRka61q1bu8zMTFdeXh6w79oc9+dySeobb7zh+vbt66644goXEhLiLrvsMtevX78zfl4bA59ztbw2C4DuvPNO5efn67333mvopQB1grePgFpyzmnDhg1asmRJQy8FqDOcKQAADFcfAQAMUQAAGKIAADBEAQBgan31UX3dZgAAUDdqc10RZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJimDb0AAN9OzZs3D2rummuu8Txz3333eZ555513PM+8+OKLnmcuNJwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBguCEecJEICwvzPDN06FDPMykpKZ5n0tLSPM/ExMR4npGkjh07BjXnVfv27T3PcEM8AECjQhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGG6IB/yf7t27e555+eWXPc845zzPSJLP5/M8E+xN57wKZm3B/jvUl8svv7yhl9AgOFMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMBwQzw0SpdddpnnmUWLFnmeadmypeeZC/1GcBe6/Px8zzO5ubmeZ5544gnPM40BZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAw3CUVjdKsWbM8zyQkJNTBSs6fBQsWeJ45depUHawk0MaNGz3PfPTRR0Ht68iRI55nCgsLg9rXtxFnCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGG6IhwtedHS055nevXt7nvH5fJ5n/vvf/3qeSU1N9TwjSQUFBUHNAV5wpgAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgOGGeLjgjRgxwvNM+/btPc845zzPvP32255nuLEdLmScKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYHyulncB8/l8db0WoFoHDhzwPBMfH18HKwlUUVHheebo0aNB7Ss9Pd3zzIoVK4LaFxqn2ny750wBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhrukot507tw5qLn169d7ngkLCwtqX14F83VRyy+5ACUlJZ5nevbs6Xlm06ZNnmdwceAuqQAAT4gCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAANO0oReAb4/x48cHNVdfN7e70AXz7zBgwADPM9wQ79uNMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAw3xMMFz+fz1ct+9uzZ43mmSRPvP1e1b9/e80yw0tLS6m1faBw4UwAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwPicc65WG9bTTcnQeMXGxgY1d/3115/nlVTv/fff9zzTrl07zzN5eXmeZ4K1fft2zzNdu3b1PHP8+HHPM6h/tfl2z5kCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACmaUMv4GIzbNgwzzPh4eGeZ5YtW+Z5RpIqKiqCmqsPhYWFQc29/fbb53kl3x5RUVGeZ8LCwupgJbhYcKYAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAw11SPcrJyfE845zzPJOQkOB5RpKef/55zzPB3r0UF778/HzPMxwP326cKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYLgh3gVq2rRpQc2lp6d7nhk7dqznmVWrVnmeQf3Ly8tr6CXgIsOZAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhhvieTR9+nTPM5MmTaqDlVQvPj7e88zKlSs9zxw7dszzzPLlyz3PSNKCBQs8zxQUFHie6d69u+eZYD63Pp/P80yw3nnnnXrbFxoHzhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADA+55yr1Yb1eBOvC1lkZKTnmUWLFnmeueOOOzzP1KdgjodaHmrnxaFDhzzPXHnllZ5n6vPfoaSkxPNMz549Pc9s2rTJ8wwuDrU59jhTAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAcEO8ehAWFuZ5plevXkHt6/e//73nmZtvvtnzTEhIiOeZ+rwhXn0J5uuiuLg4qH2NGDHC88zKlSuD2hcaJ26IBwDwhCgAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGC4Syr061//2vPMT37yE88zwdyN9UK3Z88ezzMTJkwIal8rVqwIag6oxF1SAQCeEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhhviIShhYWGeZxITE+tgJQ3r888/9zxz7NixOlgJcHbcEA8A4AlRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGC4IR4AfEtwQzwAgCdEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIBpWtsNnXN1uQ4AwAWAMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgPl/Lxqc3BnipHAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        "**Report on the results in terms of prediction accuracy on the train and test datasets:**\n",
        "- Accuracy on training set: 98.55%\n",
        "- Accuracy on test set: 96.98%"
      ],
      "metadata": {
        "id": "ysmpj4_OweD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4\n",
        "**Chosen Proposed modification:**\n",
        "-\n",
        "Raise the current count of nodes in the layer to 256\n",
        "\n",
        "**Hypothesize how it would change the performance results:**\n",
        "-\n",
        "The anticipated outcome is an enhancement in performance. By augmenting the number of units, the model's ability to represent data should improve, thereby enabling it to capture intricate relationships among the input features."
      ],
      "metadata": {
        "id": "HP3bjghfwnoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5\n",
        "Modify the model based on the chosen method and train\n",
        "\n"
      ],
      "metadata": {
        "id": "Xg9H0lenxZ_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class MLP2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP2, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the neural network model\n",
        "model = MLP2()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training Finished')\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the training set: {round(correct / total * 100, 3)}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the test set: {round(correct / total * 100, 3)}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fo7OPToxcOd",
        "outputId": "835df705-1634-4ed9-c344-13053c95d878"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.8142573830485343\n",
            "Epoch 1, Batch 200, Loss: 0.38304812625050544\n",
            "Epoch 1, Batch 300, Loss: 0.3443899601697922\n",
            "Epoch 1, Batch 400, Loss: 0.27329949356615546\n",
            "Epoch 1, Batch 500, Loss: 0.26016170635819436\n",
            "Epoch 1, Batch 600, Loss: 0.21697482354938985\n",
            "Epoch 1, Batch 700, Loss: 0.20977485336363316\n",
            "Epoch 1, Batch 800, Loss: 0.21022147580981254\n",
            "Epoch 1, Batch 900, Loss: 0.1896942576393485\n",
            "Epoch 2, Batch 100, Loss: 0.15109093025326728\n",
            "Epoch 2, Batch 200, Loss: 0.15907142285257578\n",
            "Epoch 2, Batch 300, Loss: 0.13978375149890782\n",
            "Epoch 2, Batch 400, Loss: 0.15137547831982373\n",
            "Epoch 2, Batch 500, Loss: 0.13757030822336674\n",
            "Epoch 2, Batch 600, Loss: 0.14106159966439008\n",
            "Epoch 2, Batch 700, Loss: 0.13918499274179338\n",
            "Epoch 2, Batch 800, Loss: 0.13019672829657794\n",
            "Epoch 2, Batch 900, Loss: 0.12200108909979462\n",
            "Epoch 3, Batch 100, Loss: 0.10237119130790234\n",
            "Epoch 3, Batch 200, Loss: 0.11227866601198912\n",
            "Epoch 3, Batch 300, Loss: 0.10925579791888594\n",
            "Epoch 3, Batch 400, Loss: 0.09667676784098149\n",
            "Epoch 3, Batch 500, Loss: 0.10212984702549875\n",
            "Epoch 3, Batch 600, Loss: 0.10991025317460298\n",
            "Epoch 3, Batch 700, Loss: 0.11121401930227875\n",
            "Epoch 3, Batch 800, Loss: 0.11797471245750785\n",
            "Epoch 3, Batch 900, Loss: 0.09198379254899919\n",
            "Epoch 4, Batch 100, Loss: 0.08225686996709555\n",
            "Epoch 4, Batch 200, Loss: 0.09222195974551141\n",
            "Epoch 4, Batch 300, Loss: 0.08848475567996501\n",
            "Epoch 4, Batch 400, Loss: 0.08599573532119394\n",
            "Epoch 4, Batch 500, Loss: 0.0734212841046974\n",
            "Epoch 4, Batch 600, Loss: 0.08634680015966296\n",
            "Epoch 4, Batch 700, Loss: 0.08793188220821321\n",
            "Epoch 4, Batch 800, Loss: 0.09299522567540407\n",
            "Epoch 4, Batch 900, Loss: 0.10068688096478581\n",
            "Epoch 5, Batch 100, Loss: 0.07647372207604348\n",
            "Epoch 5, Batch 200, Loss: 0.07301896874792874\n",
            "Epoch 5, Batch 300, Loss: 0.07569638166110963\n",
            "Epoch 5, Batch 400, Loss: 0.07228987051174045\n",
            "Epoch 5, Batch 500, Loss: 0.06466157448943705\n",
            "Epoch 5, Batch 600, Loss: 0.06720190515508875\n",
            "Epoch 5, Batch 700, Loss: 0.08079553180374205\n",
            "Epoch 5, Batch 800, Loss: 0.06035058195702732\n",
            "Epoch 5, Batch 900, Loss: 0.07836474702693522\n",
            "Epoch 6, Batch 100, Loss: 0.05959795999340713\n",
            "Epoch 6, Batch 200, Loss: 0.05752571775112301\n",
            "Epoch 6, Batch 300, Loss: 0.07153208713978529\n",
            "Epoch 6, Batch 400, Loss: 0.059141043091658504\n",
            "Epoch 6, Batch 500, Loss: 0.05962044769898057\n",
            "Epoch 6, Batch 600, Loss: 0.060529931760393084\n",
            "Epoch 6, Batch 700, Loss: 0.06649810425587929\n",
            "Epoch 6, Batch 800, Loss: 0.06327125951997005\n",
            "Epoch 6, Batch 900, Loss: 0.06871008213609457\n",
            "Epoch 7, Batch 100, Loss: 0.04895371230319143\n",
            "Epoch 7, Batch 200, Loss: 0.05927808185340837\n",
            "Epoch 7, Batch 300, Loss: 0.05429481401341036\n",
            "Epoch 7, Batch 400, Loss: 0.05106771183258388\n",
            "Epoch 7, Batch 500, Loss: 0.05343605192610994\n",
            "Epoch 7, Batch 600, Loss: 0.052896055330056695\n",
            "Epoch 7, Batch 700, Loss: 0.05626044328324497\n",
            "Epoch 7, Batch 800, Loss: 0.0603294047084637\n",
            "Epoch 7, Batch 900, Loss: 0.056151573188835756\n",
            "Epoch 8, Batch 100, Loss: 0.044785957939457145\n",
            "Epoch 8, Batch 200, Loss: 0.0516213377378881\n",
            "Epoch 8, Batch 300, Loss: 0.044586499590659516\n",
            "Epoch 8, Batch 400, Loss: 0.045432280814275146\n",
            "Epoch 8, Batch 500, Loss: 0.0581386874045711\n",
            "Epoch 8, Batch 600, Loss: 0.049673206333536654\n",
            "Epoch 8, Batch 700, Loss: 0.05075380545167718\n",
            "Epoch 8, Batch 800, Loss: 0.051996318687452005\n",
            "Epoch 8, Batch 900, Loss: 0.05750461504794657\n",
            "Epoch 9, Batch 100, Loss: 0.04498691576649435\n",
            "Epoch 9, Batch 200, Loss: 0.037101359417429196\n",
            "Epoch 9, Batch 300, Loss: 0.05243086266680621\n",
            "Epoch 9, Batch 400, Loss: 0.059428661125712094\n",
            "Epoch 9, Batch 500, Loss: 0.040076943276217206\n",
            "Epoch 9, Batch 600, Loss: 0.04441811356577091\n",
            "Epoch 9, Batch 700, Loss: 0.05281642542802729\n",
            "Epoch 9, Batch 800, Loss: 0.05069810239016079\n",
            "Epoch 9, Batch 900, Loss: 0.04074784160242416\n",
            "Epoch 10, Batch 100, Loss: 0.030309304283000528\n",
            "Epoch 10, Batch 200, Loss: 0.04492642185883597\n",
            "Epoch 10, Batch 300, Loss: 0.03513501730456483\n",
            "Epoch 10, Batch 400, Loss: 0.0397045357467141\n",
            "Epoch 10, Batch 500, Loss: 0.03762550200190162\n",
            "Epoch 10, Batch 600, Loss: 0.03611382189788856\n",
            "Epoch 10, Batch 700, Loss: 0.04081393349188147\n",
            "Epoch 10, Batch 800, Loss: 0.0484578117961064\n",
            "Epoch 10, Batch 900, Loss: 0.05698930998274591\n",
            "Training Finished\n",
            "\n",
            "Accuracy on the training set: 98.925%\n",
            "\n",
            "Accuracy on the test set: 97.423%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6\n",
        "**Report on the results of the modified model and if it matches your hypothesis:**\n",
        "- Accuracy on training set: 98.925%\n",
        "- Accuracy on test set: 97.423%\n",
        "\n",
        "The test accuracy demonstrates a slight improvement in this modified model, primarily attributed to the augmentation in the number of units within the hidden layers."
      ],
      "metadata": {
        "id": "ObUrzE2lzAon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7\n",
        "\n",
        "I iteratively tested different optimizers, loss functions, dropout rates, and activation functions, monitoring their impact on performance. Each experiment informed subsequent adjustments to hyperparameters, allowing for a refined optimization process."
      ],
      "metadata": {
        "id": "JjA08qhHz__Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1\n",
        "- Using LeakyRELU as the activation function"
      ],
      "metadata": {
        "id": "8GsTuKgz0S4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class MLP3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP3, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        activation = nn.LeakyReLU(0.1)\n",
        "        x = activation(self.fc1(x))\n",
        "        x = activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the neural network model\n",
        "model = MLP3()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training Finished')\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the training set: {round(correct / total * 100, 3)}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the test set: {round(correct / total * 100, 3)}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtUiqAOU0Alr",
        "outputId": "f2a0fc5b-e096-4ad4-f4e9-95abcd8a5412"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.7687590178847313\n",
            "Epoch 1, Batch 200, Loss: 0.3944550859183073\n",
            "Epoch 1, Batch 300, Loss: 0.3148936751484871\n",
            "Epoch 1, Batch 400, Loss: 0.265697428137064\n",
            "Epoch 1, Batch 500, Loss: 0.25926123015582564\n",
            "Epoch 1, Batch 600, Loss: 0.22099491845816374\n",
            "Epoch 1, Batch 700, Loss: 0.21797717727720736\n",
            "Epoch 1, Batch 800, Loss: 0.19821106042712927\n",
            "Epoch 1, Batch 900, Loss: 0.17908367140218615\n",
            "Epoch 2, Batch 100, Loss: 0.1681343640573323\n",
            "Epoch 2, Batch 200, Loss: 0.14369015593081713\n",
            "Epoch 2, Batch 300, Loss: 0.15844985714182258\n",
            "Epoch 2, Batch 400, Loss: 0.1300379884429276\n",
            "Epoch 2, Batch 500, Loss: 0.13224038464948534\n",
            "Epoch 2, Batch 600, Loss: 0.12853086613118647\n",
            "Epoch 2, Batch 700, Loss: 0.146430259142071\n",
            "Epoch 2, Batch 800, Loss: 0.12741989800706505\n",
            "Epoch 2, Batch 900, Loss: 0.135161196552217\n",
            "Epoch 3, Batch 100, Loss: 0.10713835161179304\n",
            "Epoch 3, Batch 200, Loss: 0.10211492996662855\n",
            "Epoch 3, Batch 300, Loss: 0.11240353694185615\n",
            "Epoch 3, Batch 400, Loss: 0.11758188631385565\n",
            "Epoch 3, Batch 500, Loss: 0.11153966145589948\n",
            "Epoch 3, Batch 600, Loss: 0.11304116181563587\n",
            "Epoch 3, Batch 700, Loss: 0.09158994967117906\n",
            "Epoch 3, Batch 800, Loss: 0.10180806388612837\n",
            "Epoch 3, Batch 900, Loss: 0.09714347393717616\n",
            "Epoch 4, Batch 100, Loss: 0.07978244374506176\n",
            "Epoch 4, Batch 200, Loss: 0.09556486899964511\n",
            "Epoch 4, Batch 300, Loss: 0.09629445811267942\n",
            "Epoch 4, Batch 400, Loss: 0.08730638879584149\n",
            "Epoch 4, Batch 500, Loss: 0.0795390819851309\n",
            "Epoch 4, Batch 600, Loss: 0.1072175300307572\n",
            "Epoch 4, Batch 700, Loss: 0.09052348472643644\n",
            "Epoch 4, Batch 800, Loss: 0.07677007349207997\n",
            "Epoch 4, Batch 900, Loss: 0.0819985715020448\n",
            "Epoch 5, Batch 100, Loss: 0.0680313283763826\n",
            "Epoch 5, Batch 200, Loss: 0.07238104049582034\n",
            "Epoch 5, Batch 300, Loss: 0.06523713395465165\n",
            "Epoch 5, Batch 400, Loss: 0.07096524563385173\n",
            "Epoch 5, Batch 500, Loss: 0.07232770938891918\n",
            "Epoch 5, Batch 600, Loss: 0.081611972223036\n",
            "Epoch 5, Batch 700, Loss: 0.093030316978693\n",
            "Epoch 5, Batch 800, Loss: 0.07065065325703472\n",
            "Epoch 5, Batch 900, Loss: 0.07676253535784781\n",
            "Epoch 6, Batch 100, Loss: 0.07016132627613843\n",
            "Epoch 6, Batch 200, Loss: 0.05858771969913505\n",
            "Epoch 6, Batch 300, Loss: 0.06804810022935272\n",
            "Epoch 6, Batch 400, Loss: 0.06796600496629253\n",
            "Epoch 6, Batch 500, Loss: 0.06194068842101842\n",
            "Epoch 6, Batch 600, Loss: 0.0661197752179578\n",
            "Epoch 6, Batch 700, Loss: 0.06777696072356776\n",
            "Epoch 6, Batch 800, Loss: 0.05543642411474139\n",
            "Epoch 6, Batch 900, Loss: 0.07956014736089856\n",
            "Epoch 7, Batch 100, Loss: 0.04232935449806973\n",
            "Epoch 7, Batch 200, Loss: 0.04691467259952333\n",
            "Epoch 7, Batch 300, Loss: 0.04772185917943716\n",
            "Epoch 7, Batch 400, Loss: 0.04986985509283841\n",
            "Epoch 7, Batch 500, Loss: 0.05351552659180015\n",
            "Epoch 7, Batch 600, Loss: 0.05817376267979853\n",
            "Epoch 7, Batch 700, Loss: 0.058491197844268754\n",
            "Epoch 7, Batch 800, Loss: 0.07128965545678512\n",
            "Epoch 7, Batch 900, Loss: 0.06609207583125681\n",
            "Epoch 8, Batch 100, Loss: 0.047052837123628705\n",
            "Epoch 8, Batch 200, Loss: 0.05144148887542542\n",
            "Epoch 8, Batch 300, Loss: 0.049060303240548817\n",
            "Epoch 8, Batch 400, Loss: 0.05800523664103821\n",
            "Epoch 8, Batch 500, Loss: 0.06693458596710115\n",
            "Epoch 8, Batch 600, Loss: 0.0664177789236419\n",
            "Epoch 8, Batch 700, Loss: 0.04948218087316491\n",
            "Epoch 8, Batch 800, Loss: 0.05606278327060863\n",
            "Epoch 8, Batch 900, Loss: 0.060342320760246364\n",
            "Epoch 9, Batch 100, Loss: 0.05151887140003964\n",
            "Epoch 9, Batch 200, Loss: 0.039078376388642935\n",
            "Epoch 9, Batch 300, Loss: 0.05092742674751207\n",
            "Epoch 9, Batch 400, Loss: 0.046885417066514494\n",
            "Epoch 9, Batch 500, Loss: 0.0421507723571267\n",
            "Epoch 9, Batch 600, Loss: 0.04837138798786327\n",
            "Epoch 9, Batch 700, Loss: 0.0482383093086537\n",
            "Epoch 9, Batch 800, Loss: 0.05494569727685303\n",
            "Epoch 9, Batch 900, Loss: 0.05142374752642354\n",
            "Epoch 10, Batch 100, Loss: 0.037100590664194896\n",
            "Epoch 10, Batch 200, Loss: 0.044566729220096024\n",
            "Epoch 10, Batch 300, Loss: 0.04577286632673349\n",
            "Epoch 10, Batch 400, Loss: 0.05297352413879707\n",
            "Epoch 10, Batch 500, Loss: 0.04963712525641313\n",
            "Epoch 10, Batch 600, Loss: 0.039647619914612735\n",
            "Epoch 10, Batch 700, Loss: 0.05185690884711221\n",
            "Epoch 10, Batch 800, Loss: 0.03911426372360438\n",
            "Epoch 10, Batch 900, Loss: 0.05037942139315419\n",
            "Training Finished\n",
            "\n",
            "Accuracy on the training set: 98.278%\n",
            "\n",
            "Accuracy on the test set: 96.785%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1, Observation: There is a minimal change in model performance.**"
      ],
      "metadata": {
        "id": "nkJTHHjC1k-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2\n",
        "- Adding dropout\n",
        "- Using RELU as activation function"
      ],
      "metadata": {
        "id": "cuZjgQUU1qyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class MLP4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP4, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the neural network model\n",
        "model = MLP4()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training Finished')\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the training set: {round(correct / total * 100, 3)}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the test set: {round(correct / total * 100, 3)}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag2DbzQO1gyj",
        "outputId": "e995ebef-224f-4869-c73a-e89e710b8279"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.094520983695984\n",
            "Epoch 1, Batch 200, Loss: 0.5558154931664467\n",
            "Epoch 1, Batch 300, Loss: 0.4753473436832428\n",
            "Epoch 1, Batch 400, Loss: 0.41381761148571966\n",
            "Epoch 1, Batch 500, Loss: 0.40914664216339586\n",
            "Epoch 1, Batch 600, Loss: 0.3486849971115589\n",
            "Epoch 1, Batch 700, Loss: 0.356544294282794\n",
            "Epoch 1, Batch 800, Loss: 0.325548752695322\n",
            "Epoch 1, Batch 900, Loss: 0.3339158549904823\n",
            "Epoch 2, Batch 100, Loss: 0.29641877360641955\n",
            "Epoch 2, Batch 200, Loss: 0.29483731277287006\n",
            "Epoch 2, Batch 300, Loss: 0.2881804385781288\n",
            "Epoch 2, Batch 400, Loss: 0.27488680273294447\n",
            "Epoch 2, Batch 500, Loss: 0.29092188000679015\n",
            "Epoch 2, Batch 600, Loss: 0.28439061254262926\n",
            "Epoch 2, Batch 700, Loss: 0.2682033259794116\n",
            "Epoch 2, Batch 800, Loss: 0.24329226296395062\n",
            "Epoch 2, Batch 900, Loss: 0.2577632188051939\n",
            "Epoch 3, Batch 100, Loss: 0.23342990174889564\n",
            "Epoch 3, Batch 200, Loss: 0.24786084610968828\n",
            "Epoch 3, Batch 300, Loss: 0.2475956492498517\n",
            "Epoch 3, Batch 400, Loss: 0.21764727402478456\n",
            "Epoch 3, Batch 500, Loss: 0.23501949571073055\n",
            "Epoch 3, Batch 600, Loss: 0.23003680974245072\n",
            "Epoch 3, Batch 700, Loss: 0.22711771849542856\n",
            "Epoch 3, Batch 800, Loss: 0.23374510917812585\n",
            "Epoch 3, Batch 900, Loss: 0.22396442472934722\n",
            "Epoch 4, Batch 100, Loss: 0.2144810903817415\n",
            "Epoch 4, Batch 200, Loss: 0.2287484272569418\n",
            "Epoch 4, Batch 300, Loss: 0.2239955769479275\n",
            "Epoch 4, Batch 400, Loss: 0.20855777740478515\n",
            "Epoch 4, Batch 500, Loss: 0.21170863036066292\n",
            "Epoch 4, Batch 600, Loss: 0.2061453391239047\n",
            "Epoch 4, Batch 700, Loss: 0.228402864895761\n",
            "Epoch 4, Batch 800, Loss: 0.1962829713523388\n",
            "Epoch 4, Batch 900, Loss: 0.20891151160001756\n",
            "Epoch 5, Batch 100, Loss: 0.19257459450513126\n",
            "Epoch 5, Batch 200, Loss: 0.18381334913894534\n",
            "Epoch 5, Batch 300, Loss: 0.218684121966362\n",
            "Epoch 5, Batch 400, Loss: 0.19886336658149958\n",
            "Epoch 5, Batch 500, Loss: 0.20385508593171836\n",
            "Epoch 5, Batch 600, Loss: 0.22972458928823472\n",
            "Epoch 5, Batch 700, Loss: 0.19028867926448584\n",
            "Epoch 5, Batch 800, Loss: 0.19008670423179866\n",
            "Epoch 5, Batch 900, Loss: 0.20116691172122955\n",
            "Epoch 6, Batch 100, Loss: 0.1793871002085507\n",
            "Epoch 6, Batch 200, Loss: 0.19125508593395352\n",
            "Epoch 6, Batch 300, Loss: 0.2013792535290122\n",
            "Epoch 6, Batch 400, Loss: 0.18768381752073765\n",
            "Epoch 6, Batch 500, Loss: 0.18262941516935827\n",
            "Epoch 6, Batch 600, Loss: 0.19671812266111374\n",
            "Epoch 6, Batch 700, Loss: 0.17165752846747637\n",
            "Epoch 6, Batch 800, Loss: 0.19167618811130524\n",
            "Epoch 6, Batch 900, Loss: 0.19422236505895854\n",
            "Epoch 7, Batch 100, Loss: 0.17656498197466136\n",
            "Epoch 7, Batch 200, Loss: 0.16734739519655706\n",
            "Epoch 7, Batch 300, Loss: 0.16935692008584738\n",
            "Epoch 7, Batch 400, Loss: 0.17970525100827217\n",
            "Epoch 7, Batch 500, Loss: 0.18839425750076771\n",
            "Epoch 7, Batch 600, Loss: 0.17307263117283583\n",
            "Epoch 7, Batch 700, Loss: 0.1859919286891818\n",
            "Epoch 7, Batch 800, Loss: 0.17379757205024363\n",
            "Epoch 7, Batch 900, Loss: 0.18117455542087554\n",
            "Epoch 8, Batch 100, Loss: 0.18353854244574905\n",
            "Epoch 8, Batch 200, Loss: 0.18589268386363983\n",
            "Epoch 8, Batch 300, Loss: 0.1730741671845317\n",
            "Epoch 8, Batch 400, Loss: 0.17022119097411634\n",
            "Epoch 8, Batch 500, Loss: 0.1621578834205866\n",
            "Epoch 8, Batch 600, Loss: 0.18082822617143393\n",
            "Epoch 8, Batch 700, Loss: 0.16541538892313837\n",
            "Epoch 8, Batch 800, Loss: 0.150151525195688\n",
            "Epoch 8, Batch 900, Loss: 0.1789686055481434\n",
            "Epoch 9, Batch 100, Loss: 0.1582954303920269\n",
            "Epoch 9, Batch 200, Loss: 0.1675289038196206\n",
            "Epoch 9, Batch 300, Loss: 0.15539607707411052\n",
            "Epoch 9, Batch 400, Loss: 0.15260371565818787\n",
            "Epoch 9, Batch 500, Loss: 0.1835455136746168\n",
            "Epoch 9, Batch 600, Loss: 0.17058990428224205\n",
            "Epoch 9, Batch 700, Loss: 0.15702032729983328\n",
            "Epoch 9, Batch 800, Loss: 0.18524577766656875\n",
            "Epoch 9, Batch 900, Loss: 0.1583538679033518\n",
            "Epoch 10, Batch 100, Loss: 0.1532193635031581\n",
            "Epoch 10, Batch 200, Loss: 0.16207882326096296\n",
            "Epoch 10, Batch 300, Loss: 0.1642709319293499\n",
            "Epoch 10, Batch 400, Loss: 0.1727889825962484\n",
            "Epoch 10, Batch 500, Loss: 0.17612583020702005\n",
            "Epoch 10, Batch 600, Loss: 0.16720949618145822\n",
            "Epoch 10, Batch 700, Loss: 0.14677226839587093\n",
            "Epoch 10, Batch 800, Loss: 0.1490537429228425\n",
            "Epoch 10, Batch 900, Loss: 0.16729108519852162\n",
            "Training Finished\n",
            "\n",
            "Accuracy on the training set: 97.738%\n",
            "\n",
            "Accuracy on the test set: 96.852%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Experiment 2, Observation: The alteration in model performance remains minimal, mirroring the outcomes observed in experiment 1."
      ],
      "metadata": {
        "id": "7h4rnDX45owu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2\n",
        "- Optimizer : SGD with Momentum\n",
        "- Remove Dropout\n",
        "- Keep Loss Function as it is. (CrossEntropyLoss is a good loss function for image classification tasks)\n",
        "- Change epochs to 10"
      ],
      "metadata": {
        "id": "BZ5utU5F5tzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class MLP5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP5, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the neural network model\n",
        "model = MLP5()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training Finished')\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the training set: {round(correct / total * 100, 3)}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'\\nAccuracy on the test set: {round(correct / total * 100, 3)}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMvOKjwh5r7W",
        "outputId": "ecc528fa-d5be-4254-a519-0f42e15e79bd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.2344424057006838\n",
            "Epoch 1, Batch 200, Loss: 1.96050039768219\n",
            "Epoch 1, Batch 300, Loss: 1.4477262008190155\n",
            "Epoch 1, Batch 400, Loss: 0.9816925448179245\n",
            "Epoch 1, Batch 500, Loss: 0.7374842995405197\n",
            "Epoch 1, Batch 600, Loss: 0.6112575918436051\n",
            "Epoch 1, Batch 700, Loss: 0.533815955221653\n",
            "Epoch 1, Batch 800, Loss: 0.49921472132205963\n",
            "Epoch 1, Batch 900, Loss: 0.4432837899029255\n",
            "Epoch 2, Batch 100, Loss: 0.41800090461969375\n",
            "Epoch 2, Batch 200, Loss: 0.41832919970154764\n",
            "Epoch 2, Batch 300, Loss: 0.39498530834913254\n",
            "Epoch 2, Batch 400, Loss: 0.3740631528198719\n",
            "Epoch 2, Batch 500, Loss: 0.35637629121541975\n",
            "Epoch 2, Batch 600, Loss: 0.35862250469624996\n",
            "Epoch 2, Batch 700, Loss: 0.34630589321255684\n",
            "Epoch 2, Batch 800, Loss: 0.3434796267747879\n",
            "Epoch 2, Batch 900, Loss: 0.3426695528626442\n",
            "Epoch 3, Batch 100, Loss: 0.3273366059362888\n",
            "Epoch 3, Batch 200, Loss: 0.322786949723959\n",
            "Epoch 3, Batch 300, Loss: 0.32061602890491486\n",
            "Epoch 3, Batch 400, Loss: 0.3287878656387329\n",
            "Epoch 3, Batch 500, Loss: 0.3300999970734119\n",
            "Epoch 3, Batch 600, Loss: 0.3226958164572716\n",
            "Epoch 3, Batch 700, Loss: 0.29894636660814283\n",
            "Epoch 3, Batch 800, Loss: 0.30427762180566786\n",
            "Epoch 3, Batch 900, Loss: 0.2903685460984707\n",
            "Epoch 4, Batch 100, Loss: 0.2755767890065908\n",
            "Epoch 4, Batch 200, Loss: 0.3089458145201206\n",
            "Epoch 4, Batch 300, Loss: 0.28831457734107974\n",
            "Epoch 4, Batch 400, Loss: 0.29582385808229444\n",
            "Epoch 4, Batch 500, Loss: 0.27750451646745206\n",
            "Epoch 4, Batch 600, Loss: 0.29562299117445945\n",
            "Epoch 4, Batch 700, Loss: 0.27011153638362884\n",
            "Epoch 4, Batch 800, Loss: 0.2650825396180153\n",
            "Epoch 4, Batch 900, Loss: 0.27028387002646925\n",
            "Epoch 5, Batch 100, Loss: 0.2752138938754797\n",
            "Epoch 5, Batch 200, Loss: 0.25051131516695024\n",
            "Epoch 5, Batch 300, Loss: 0.2604912390559912\n",
            "Epoch 5, Batch 400, Loss: 0.25393418692052366\n",
            "Epoch 5, Batch 500, Loss: 0.252280318364501\n",
            "Epoch 5, Batch 600, Loss: 0.2561108445376158\n",
            "Epoch 5, Batch 700, Loss: 0.25581894524395465\n",
            "Epoch 5, Batch 800, Loss: 0.242730867639184\n",
            "Epoch 5, Batch 900, Loss: 0.26473023124039174\n",
            "Epoch 6, Batch 100, Loss: 0.2449720489233732\n",
            "Epoch 6, Batch 200, Loss: 0.2627021573483944\n",
            "Epoch 6, Batch 300, Loss: 0.23065594598650932\n",
            "Epoch 6, Batch 400, Loss: 0.2375595396012068\n",
            "Epoch 6, Batch 500, Loss: 0.24756639391183854\n",
            "Epoch 6, Batch 600, Loss: 0.2395830062031746\n",
            "Epoch 6, Batch 700, Loss: 0.2185135693848133\n",
            "Epoch 6, Batch 800, Loss: 0.21869189165532588\n",
            "Epoch 6, Batch 900, Loss: 0.23095509842038153\n",
            "Epoch 7, Batch 100, Loss: 0.23237042114138604\n",
            "Epoch 7, Batch 200, Loss: 0.22471516005694867\n",
            "Epoch 7, Batch 300, Loss: 0.21435790210962297\n",
            "Epoch 7, Batch 400, Loss: 0.22143795363605023\n",
            "Epoch 7, Batch 500, Loss: 0.20399113722145557\n",
            "Epoch 7, Batch 600, Loss: 0.21128349971026184\n",
            "Epoch 7, Batch 700, Loss: 0.20160237204283477\n",
            "Epoch 7, Batch 800, Loss: 0.2132657142728567\n",
            "Epoch 7, Batch 900, Loss: 0.2112111333757639\n",
            "Epoch 8, Batch 100, Loss: 0.2053899169713259\n",
            "Epoch 8, Batch 200, Loss: 0.20591515220701695\n",
            "Epoch 8, Batch 300, Loss: 0.2099714234471321\n",
            "Epoch 8, Batch 400, Loss: 0.20120681546628474\n",
            "Epoch 8, Batch 500, Loss: 0.20386412169784307\n",
            "Epoch 8, Batch 600, Loss: 0.18692365918308496\n",
            "Epoch 8, Batch 700, Loss: 0.18386406444013118\n",
            "Epoch 8, Batch 800, Loss: 0.19099049609154461\n",
            "Epoch 8, Batch 900, Loss: 0.18319751903414727\n",
            "Epoch 9, Batch 100, Loss: 0.19993172690272332\n",
            "Epoch 9, Batch 200, Loss: 0.18676659621298314\n",
            "Epoch 9, Batch 300, Loss: 0.18635077625513077\n",
            "Epoch 9, Batch 400, Loss: 0.16642768651247025\n",
            "Epoch 9, Batch 500, Loss: 0.1773762971162796\n",
            "Epoch 9, Batch 600, Loss: 0.18141214191913604\n",
            "Epoch 9, Batch 700, Loss: 0.17901360217481851\n",
            "Epoch 9, Batch 800, Loss: 0.1856166670843959\n",
            "Epoch 9, Batch 900, Loss: 0.1661375240236521\n",
            "Epoch 10, Batch 100, Loss: 0.17203173581510783\n",
            "Epoch 10, Batch 200, Loss: 0.17433190025389195\n",
            "Epoch 10, Batch 300, Loss: 0.16838707614690065\n",
            "Epoch 10, Batch 400, Loss: 0.16367582228034736\n",
            "Epoch 10, Batch 500, Loss: 0.16874424319714307\n",
            "Epoch 10, Batch 600, Loss: 0.15705282744020224\n",
            "Epoch 10, Batch 700, Loss: 0.16430350590497256\n",
            "Epoch 10, Batch 800, Loss: 0.17255918752402066\n",
            "Epoch 10, Batch 900, Loss: 0.16946511913090945\n",
            "Training Finished\n",
            "\n",
            "Accuracy on the training set: 95.452%\n",
            "\n",
            "Accuracy on the test set: 94.795%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Experiment 3, Observation: The observed model performance is inferior, as indicated above, underscoring the superiority of Adam over SGD with momentum in terms of efficacy."
      ],
      "metadata": {
        "id": "K2JtzTc06-ci"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQyxyCFa57Pg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}